<!DOCTYPE html>
<head>
	<meta name="author" content="Danielle Bragg" />
	<meta name="description" content="Danielle Bragg's website - Senior Researcher at Microsoft Research with interests in Human Computer Interaction, Accessibility, and Machine Learning" />
	<title>Danielle Bragg - Home </title>
	<meta name="keywords" content="Danielle Bragg, Computer Science, Human Computer Interaction, HCI, Machine Learning, ML, accessibility, Microsoft, Microsoft Research, MSR, University of Washington, UW" />
	<!--<meta name = "viewport" content = "user-scalable = yes">-->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<link rel="shortcut icon" href="images/light_transparent.png">

	<link rel="stylesheet" type="text/css" href="css/custom.css"/>

    <link href="libraries/bootstrap.min.css" rel="stylesheet">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"> </script>
	
	<script type='text/javascript'>

		
		$(document).ready(function(){
			
			$(".topnav a").click(function(){
			console.log('done!');
			$('html, body').animate({
				scrollTop: $(this.hash).offset().top - $(".topnav").height()}, 'slow');
			
			});
			
		});
	
	</script>
	
	
</head>

<body>


    <header class="intro" style="background-image: url('images/20160616_203445_tiny.png'); background-size: cover;">


			<div class="topnav">
			  <div>Danielle Bragg</div>
			  <a href="#about">About</a>
			  <a href="#research">Research</a>
			  <a href="#honors">Honors</a>
			  <a href="#publications">Publications</a>
			</div>


		<div class="intro strokeme">
			<div class="centering">
			 <img class="img-circle" src="images/dbragg_headshot_tiny.png" style="width:200px; border: 6px solid white;"> 
				<h1 class="" style="font-size:55px; font-weight:bold; color:white;">Danielle Bragg</h1>
			  <p class="intro-text" style="">Senior Researcher <br>
				Microsoft Research <br>
				danielle.bragg [at] microsoft.com 
			  </p>
			</div>

        </div>

    </header>
	
	
	<div class="container">
		
		<div class = "content">
		<br>
		<h2 class="subtitle" id="about"> ABOUT ME </h2> <br>
		
		<div class = "majorTextBox"> 
		
			<div style="width:58%; margin-right:2%; display: inline-block;">
				<p>I am a Senior Researcher at <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" target="_blank" style="cursor: pointer;">Microsoft Research New England</a>.</p>
					
				<p>My research focuses on developing systems that expand acces to information, in particular for people with disabilities. 
				My work is interdisciplinary, combining <b>Human-Computer Interaction</b>, <b>Applied Machine Learning</b>, and <b>Accessibility</b>. I completed my PhD in Computer Science at the <a dataquery="#Link14iu" href="http://www.cs.washington.edu/" target="_blank" style="cursor: pointer;">University of Washington</a> advised by <a dataquery="#Link14iu" href="http://www.cs.washington.edu/people/faculty/ladner/" target="_blank" style="cursor: pointer;"> Richard Ladner</a>, and hold an AB in Applied Mathematics from <a dataquery="#Link14iu" href="http://www.seas.harvard.edu/programs/applied-mathematics"> Harvard</a>. My work has been featured in the <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/blog/accessible-systems-for-sign-language-computation-with-dr-danielle-bragg/" target="_blank" style="cursor: pointer;">Microsoft Research Podcast</a> and in <a dataquery="#Link14iu" href="https://www.economist.com/science-and-technology/2021/03/04/the-race-to-teach-sign-language-to-computers" target="_blank" style="cursor: pointer;">The Economist</a>.</p>

				<!--
				<p>My recent work has focused largely on building systems to better support sign language users. Building these systems requires tackling a variety of challenges in data collection, modeling, visual interface design, social interactions, and beyond. To learn more, check out this <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/blog/accessible-systems-for-sign-language-computation-with-dr-danielle-bragg/" target="_blank" style="cursor: pointer;">podcast</a>, where I discuss challenges facing the field of sign language computation. My work has also been featured in <a dataquery="#Link14iu" href="https://www.economist.com/science-and-technology/2021/03/04/the-race-to-teach-sign-language-to-computers" target="_blank" style="cursor: pointer;">The Economist</a>.
				</p>
				
				<p>
				My diverse past research projects have spanned recommender systems, educational tools, data visualization, computational biology, computer music, applied mathematics, and network protocols.
				</p>-->				
				
				
			</div>
			
			<div style="width:36%; float:right;">
				<h3><b> EXPERIENCE </b> </h3>
				<table class="experience" style="width:100%">
					<tr> 
						<th style="width:20%" valign="top"> Current </th>
						<td> Senior Researcher, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr> 
						<th style="width:20%" valign="top"> 2018-2020 </th>
						<td> Postdoc, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr> 
						<th style="width:20%" valign="top"> 2013-2018 </th>
						<td> PhD Student, <a dataquery="#Link14iu" href="http://www.cs.washington.edu/" target="_blank" style="cursor: pointer;">University of Washington</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2015, 2016 </th>
						<td> Intern, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2014 </th>
						<td> Intern, <a dataquery="#Link14iu" href="http://www.bing.com/">Microsoft Bing</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2011-2012 </th>
						<td> PhD Student, <a dataquery="#Link14iu" href="http://www.cs.princeton.edu/" targt="_blank" style="cursor: pointer;"> Princeton </a></td>
					</tr>
					<tr>
						<th valign="top"> 2010-2011 </th>
						<td> Research Assistant, <a dataquery="#Link14iu" href="http://www.cs.seas.gwu.edu/networking-and-mobile-computing"> George Washington University</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2010 </th>
						<td> AB, Applied Mathematics, <i> cum laude</i>, <a dataquery="#Link14iu" href="http://www.seas.harvard.edu/programs/applied-mathematics"> Harvard </a> </td>
					</tr>
				</table>
				
				<p class="experience"> For more details, please see my <a href="DBragg_CV.pdf">CV</a>. </p>
			</div>
		</div>
		

		<h2 class = "subtitle" id="research"> RESEARCH </h2> <br>
		
		<div class="majorTextBox">
		
			I build systems that improve access to information by leveraging modern computing capabilities in innovative ways. My work combines human-computer interaction (HCI) with applied machine learning, in collaboration with interdisciplinary colleagues from computer vision, machine learning, graphics, linguistics, typography, and the social sciences. My work employs both quantitative and qualitative methods, using data collected through crowdsourcing platforms to iterate on designs, explore solution spaces, and solve data scarcity problems. I partner with target users to incorporate input and build community trust, and am committed to publicly releasing projects so those users can benefit.
			
			<!--My research seeks to make information more available, in particular to sign language users and low-vision readers. As interactive technologies become increasingly rich, multi-modal, and pervasive, ensuring access becomes increasingly important and challenging. At the same time, new technologies and machine learning techniques offer new capabilities that can improve accessibility, and introduce opportunities for innovation. My work seeks to leverage these opportunities to help address barriers.-->
			
		</div>
		<br>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of ASL Sea Battle app, showing a signer at the top and a grid of tiles below" src="images/bs_label_confirm_crop.png" ></img>
				<h3 class = "subsubtitle"> ASL SEA BATTLE - </h3>
				Lack of data is the biggest barrier to developing intelligent sign language systems. To collect larger, more representative AI datasets, we propose a sign language game. Our game is a variant of the traditional battleships game. In our ASL version, grid squares are labelled with signs, and players guess by recording a sign, thereby contributing to the dataset. The opponent views the video and taps the matching square to reveal a response, thereby providing a label. The game also provides a fun and educational sign language resource. <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/ASL_sea_battle_demo.mp4">recorded demo</a>, <a href="presentations/ASL Sea Battle.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of a signer with a tiger head avatar replacing their head" src="images/tiger pic signing crop.png" ></img>
				<h3 class = "subsubtitle"> SIGN LANGUAGE ML PRIVACY - </h3>
				Sign language data is intrinsically personal -- typically consisting of videos of people signing, which capture the face, body, and surroundings. We propose applying filters to these videos to help address privacy concerns, boost dataset participation, and thereby potentially boost ML model performance as well. We experimented with various filters applied to the signer and surroundings, and ran both user studies and computer vision experiments to validate this idea. <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/exploring collection.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Diagram of a wearable device, which consists of a baseball hat with an attached 3D-printed piece hosting a speaker/microphone, smart glasses, and camera with fisheye lens" src="images/chat in the hat.png" ></img>
				<h3 class = "subsubtitle"> CHAT IN THE HAT - </h3>
				Virtual remote interpreting (VRI) makes sign language interpretations available through a video call. VRI requires the signer to hold a smartphone (constraining signing and multitasking), or to place the video call device in a fixed location (constraining mobility). We prototyped a wearable device to free the signer's hands and enable mobility while using VRI. The device consists of a baseball hat with a speaker/mic (English input/output), mounted fisheye camera (ASL input), and smart glasses displaying the interpreter (ASL output).  <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/Chat in the hat.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="The word livefonts in several scripts, including an animated script" src="images/livefonts_crop.gif" ></img>
				<h3 class = "subsubtitle"> SMARTFONTS - </h3>
				By redesigning English letterforms, smartfonts and livefonts challenge our assumption that we should read traditional letterforms on modern screens. Personal devices enable this challenge, by allowing users to adopt new letterforms without language reform or mass adoption. While smartfonts leverage color, shape, and spacing, livefonts add animation to the design space. Potential benefits include increased legibility, privacy, aesthetics, and fun. Try it out by downloading <a href="images/Visibraille.ttf">a smartfont</a> and uploading it to Chrome's <a href="https://chrome.google.com/webstore/detail/font-changer/obgkjikcnonokgaiablbenkgjcdbknna?hl=en">Font Changer</a> extension, to render all your web content in a smartfont!<div style="line-height:50%"><br></div>
				
				More: <a href="smartfonts.html">interactive demo</a>, <a href="Smartfonts.mp4">smartfonts presentation</a>, <a href="presentations/Livefonts.mp4">livefonts presentation</a>, smartfonts paper below, livefonts paper below
		</div>
		
		
		<div class = "majorTextBox">
		
			<!--<img class="projectImage_original" alt="The word livefonts in several scripts, including an animated script" src="images/livefonts_crop.gif" ></img>-->
		
			<img class="projectImage_original" alt="an animated ASL character for the sign UNDERSTAND" src="images/understand.gif" style="padding-right: 35px; padding-left: 35px;"></img>
				<div class = "subsubtitle"> ANIMATED SI5S - </div>
				While ASL character/writing systems offer many potential benefits, none have been widely adopted. Time and effort to learn them is one obstacle to adoption. All past ASL scripts have been stationary (non-animated), which makes depicting movement difficult. We capitalize on modern animation capabilities, proposing the first animated ASL character system prototype. By representing movement iconically, our system reduces the need to memorize symbolic representations of complex movements.<div style="line-height:50%"><br></div>
				
				More: <a href="animated_si5s.html">interactive demo</a>, paper below
		</div>
		

		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Workflow of ASL-Search" src="images/asl-search_interface.png"></img>
			<h3 class = "subsubtitle"> ASL-SEARCH -  </h3>
			ASL-Search is an ASL dictionary that lets students look up the English meanings of signs. Looking up the meaning of a sign is difficult because a sign is a 3D movement not easily described with written words. Our dictionary lets users enter a set of features including hand shape, location, and movement, to describe a sign and look it up. The dictionary learns from the features that previous users enter to improve results for future users.<div style="line-height:50%"><br></div>
				
				More: <a dataquery="#Link14iu" href="https://asltoenglish.org" target="_blank" style="cursor: pointer;">https://asltoenglish.org</a> (beta release), paper below
		</div>
		
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Workflow of ASL-Search" src="images/asl-flash_descriptive.png" width="100px" style="float:left; outline: black solid thin; margin-right:10px"></img>
			
			<h3 class = "subsubtitle"> ASL-FLASH - </h3>
			
			ASL-Flash is a site that both helps people learn American Sign Language and provides featural descriptions of signs. The site provides "flashcards" of signs, showing visitors videos of signs and quizzing them on the English meanings and compositional features (e.g., handshape and hand location). The data that users provide helps us build the ASL-Search dictionary. Check it out at <a dataquery="#Link14iu" href="http://www.aslflash.org" target="_blank" style="cursor: pointer;"> www.aslflash.org </a> and learn some signs!<div style="line-height:50%"><br></div>
				
				More: <a dataquery="#Link14iu" href="https://aslflash.org" target="_blank" style="cursor: pointer;">https://aslflash.org</a>, paper below
		</div>
		
		
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of the study, with the words 'Press play when you are ready' and a large play button" src="images/listening_rates_play.png"></img>
			

			<h3 class = "subsubtitle"> LISTENING RATES - </h3>
		
			We provide the first inclusive, large-scale studies on human listening rates. As conversational agents and digital assistants become increasingly pervasive, understanding their synthetic speech becomes increasingly important. Our studies presented volunteer participants with a series of challenging listening questions in order to learn more about their own listening rates. Our results inform synthetic speech rate optimization and future inclusive crowdsourced studies.
		<div style="line-height:50%"><br></div>
				
				More: initial study paper below, expanded study paper below
		</div>
		

		</div>
		
		
		<!--
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of the sound detector app, listing sounds of interest to the user" src="images/detector_small.png"></img>
			

			<h3 class = "subsubtitle"> SOUND DETECTOR - </h3>
			
			The sound detector is a trainable app that alerts users to sounds of interest (e.g., a door knock, appliance running, or alarm ringing). Sounds provide important information, and non-auditory cues are not always available. In these situations, a sound detector can be useful to deaf or hard-of-hearing people. Our mobile app design provides personalized sound awareness through a ubiquitous device. The user records examples of sounds, and the app notifies the user when they occur.

		</div>
		-->
		<!--
		<h2 class = "subtitle"> PREVIOUS STUDENTS/INTERNS </div>
		<br>
		
		Larwan Berke (RIT/Gallaudet), Abraham Glasser, 
		-->
		
		
		<h2 class = "subtitle" id="honors"> HONORS </h2>
		
		<br>
		
		<div class = "majorTextBox">
			<ul>
				<li> <a href="https://programs.sigchi.org/chi/2021/awards">Honorable Mention Award, CHI</a> (1st-author publication)</a> 2021 </li>
				<li> <a href="https://www.sigaccess.org/category/news/awards/best-paper/">Best Paper Award, ASSETS</a> (1st-author publication)</a> 2019 </li>
				<li> <a href="https://blogs.microsoft.com/accessibility/ability-summit-2020/">Microsoft Ability Summit</a>, Innovation Award Winner</a> 2019 </li>
				<li> <a href="https://blogs.microsoft.com/newengland/2018/06/29/new-england-machine-learning-accessibility/"> New England Machine Learning Accessibility Hackathon, 1st Place</a> 2018 </li>
				<li> <a href="http://www.aspirations.org/2017-ncwit-collegiate-award-recipients"> National Center for Women & Information Technology (NCWIT) Collegiate Award</a> 2017 </li>
				<li> <a href="http://expd.uw.edu/expo/scholarships/JMRS">Judith M. Runstad Wells Fargo Women's Roundtable Scholarship</a> 2017 </li>
				<li> <a href="https://disabilitystudies.washington.edu/Hahn_Lang_awards_2017">Harlan Hahn Endowment Fund Grant</a> 2017 </li>
				<li> <a href="http://www.microsoft.com/en-us/research/academic-program/womens-fellowship-program/">Microsoft Research Graduate Women's Scholarship</a> 2012</li>
			
			</ul>
		</div>
		
		
		<h2 class = "subtitle" id="publications"> SELECT PUBLICATIONS (<a dataquery="#Link14iu" href="http://scholar.google.com/citations?user=6fd-LUEAAAAJ&hl=en&oi=ao">Google Scholar</a>)</h2><br>
		
		<div class = "majorTextBox">
			<ul>
				<!--
				<li> <b>D. Bragg</b>, E. Maris. "Communication Impact Checklist (CIC): Deploying Communication Theory in HCI." In submission. (paper) </li>-->
				
				<li> <i>The FATE Landscape of Sign Language AI Datasets: An Interdisciplinary Perspective</i> <br>
				<b>D. Bragg</b>, N. Caselli, J. Hochgesang, M. Huenerfauth, L. Katz-Hernandez, O. Koller, R. Kushalnagar, C. Vogler, R. Ladner. 
				To appear, TACCESS 2021. <a href="papers/Sign_Language_Dataset_AI_FATE.pdf">(pre-print)</a> </li><br>
				
				<li> <i>Expanding a Large Inclusive Study of Human Listening Rates</i> <br>
				<b>D. Bragg</b>, K. Reinecke, R. Ladner. 
				To appear, TACCESS 2021.</li><br>
				
				<li> <i>Mixed Abilities and Varied Experiences: A Group Autoethnography of a Virtual Summer Internship</i> <br>
				K. Mack, M. Das, D. Jain, <b>D. Bragg</b>, J. Tang, A. Begel, E. Benetau, J. Davis, A. Glasser, J. Park, V. Potluri. 
				To appear, ASSETS 2021. </li><br>
				
				<li> <i>ASL Sea Battle: Gamifying Sign Language Data Collection</i> <br>
				<b>D. Bragg</b>, N. Caselli, J. W. Gallagher, M. Goldberg, C. Oka, W. Thies. CHI 2021. <span class="badge badge-award">Honorable Mention</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/ASL_Sea_Battle.pdf">(paper)</a> </li><br>
				
				<li> <i>The Promise and Peril of Parallel Chat in Video Meetings for Work</i> <br>
				A. Sarkar, S. Rintel, D. Borowiec, R. Bergmann, S. Gillett, <b>D. Bragg</b>, N. Baym, A. Sellen. CHI LBW 2021. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/02/2021_CHI_LBW___parallel_chat___author_own_version_custom_footer.pdf">(paper)</a> </li><br>
				
				<li> <i>Designing an Online Infrastructure for Collecting AI Data From People With Disabilities</i> <br>
				J. Park, <b>D. Bragg</b>, E. Kamar., M.R. Morris. FAccT 2021. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/01/Inclusive_AI_Datasets_FINAL.pdf">(paper)</a></li><br>
				
				<li> <i>Exploring Collection of Sign Language Datasets: Privacy, Participation, and Model Performance</i> <br>
				<b>D. Bragg</b>, O. Koller, N. Caselli, W. Thies. 
				ASSETS 2020. <a href="papers/Exploring_Collection_of_Sign_Language_Datasets.pdf">(paper)</a> </li><br>
				
				<li> <i>Chat in the Hat: A Portable Interpreter for Sign Language Users</i> <br> 
				L. Berke, W. Thies, <b>D. Bragg</b>.
				ASSETS 2020.  <a href="papers/Chat_in_the_Hat.pdf">(paper)</a> </li><br>
				
				<li> <i>Social App Accessibility for Deaf Signers</i> <br>
				K. Mack, <b>D. Bragg</b>, M.R. Morris, M. Boss, I. Albi, A. Monroy-Hernandez.
				CSCW 2020. <a href="papers/Social_App_Accessibility_for_Deaf_Signers.pdf">(paper)</a> </li><br>
								
				<li> <i>Sign Language Interfaces: Discussing the Field's Biggest Challenges</i> <br>
				<b>D. Bragg</b>, M.R. Morris, C. Vogler, R. Kushalnagar, M. Huenerfauth, H. Kacorri.
				CHI 2020. <a href="papers/Sign_Language_Interfaces.pdf">(extended abstract)</a> </li><br>
				
				<li> <i>Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective</i> <br>
				<b>D. Bragg</b>, O. Koller, M. Bellard, L. Berke, P. Boudrealt, A. Braffort, N. Caselli, M. Huenerfauth, H. Kacorri, T. Verhoef, C. Vogler, M.R. Morris. 
				ASSETS 2019. <span class="badge badge-award">Best Paper</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/Sign_Language_Workshop.pdf">(paper)</a> <a href="https://arxiv.org/pdf/1908.08597.pdf">(pre-print)</a> </li><br>
				
				<li> <i> Designing an Animated Character System for American Sign Language</i> <br>
				<b>D. Bragg</b>, R. Kushalnagar, R. Ladner. 
				ASSETS 2018. <a href="papers/Animated_Si5s.pdf">(paper)</a> <small> <b> <--- NOTE: Open with Adobe Reader to see animations <i>in the PDF itself!</i> </b> </small>  </li> </li><br>
									
				<li> <i> A Large Inclusive Study of Human Listening Rates</i> <br>
				<b>D. Bragg</b>, C. Bennett, K. Reinecke, R. Ladner.
				CHI 2018. <a href="papers/listening_rates.pdf">(paper)</a> </li><br>
				
				<li> <i>Designing and Evaluating Livefonts</i><br>
				 <b>D. Bragg</b>, S. Azenkot, K. Larson, A. Bessemans, A. Kalai.
				 UIST 2017. <a href="papers/livefonts.pdf">(paper)</a> <small> <b> <--- NOTE: Open with Adobe Reader to see animations <i>in the PDF itself!</i> </b> </small>  </li><br>
				
				<li> <i>Reading and Learning Smartfonts</i> <br>
				 <b>D. Bragg</b>, S. Azenkot, A. Kalai. 
				 UIST 2016. <a href="papers/Reading_and_Learning_Smartfonts.pdf">(paper)</a> </li><br>
				
				<li> <i> A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users</i><br>
				<b>D. Bragg</b>, N. Huynh, R. Ladner. 
				ASSETS 2016. <a href="papers/Personalizable_Mobile_Sound_Detector.pdf"> (paper) </a> </li><br>
				
				<li> <i> A User-Powered American Sign Language Dictionary</i><br>
				<b>D. Bragg</b>, K. Rector, R. Ladner. 
				CSCW 2015. <a href="papers/A_User-Powered_American_Sign_Language_Dictionary.pdf"> (paper) </a> </li><br>
			
				<li> <i> Synchronous Data Flow Modeling for DMIs</i><br>
				<b>D. Bragg</b>. 
				NIME 2013. <a href="papers/Synchronous_Data_Flow_Modeling.pdf" > (paper) </a> </li><br>

				<!--<li> Bragg, D., M. Yun, H. Bragg, H-A. Choi. “Intelligent Transmission of Patient Sensor Data in Wireless Hospital Networks.” Proc. AMIA (American Medical Informatics Association) Symposium. Chicago, Illinois. Nov 2012. <a href="papers/Intelligent_Transmission.pdf" > (paper) </a> </li>-->
				
				<!--<li> Bragg, D., M. Yun, H. Bragg, and H.A. Choi. “Game Theoretical Approach to Scheduling Transmission of Data for Hospital Patients.” Tel Aviv International Workshop on Game Theory. Tel Aviv, Israel. June 2011. (poster) </li>-->
				
				<li> <i> Battle Event Detection Using Sensor Networks and Distributed Query Processing</i><br>
				M. Yun, <b>D. Bragg</b>, A. Arora, and H.A. Choi.
				IEEE INFOCOM 2011. <a href="papers/Battle_Event_Detection.pdf" > (paper) </a> </li><br>
				
				<li> <i> On Data Transmission Scheduling considering Switching Penalty in Mobile Sensor Networks</i><br>
				Y. Zhou, <b>D. Bragg</b>, M. Yun, and H.A. Choi.
				IEEE INFOCOM 2011. <a href="papers/On_Data_Transmission_Scheduling.pdf" > (paper) </a> </li><br>

				<li> <i> Quantification and Display of Emotions in Music</i><br>
				<b>D. Bragg</b>. 
				Honors Senior Thesis, Harvard University Department of Applied Mathematics, 2010. <a href="papers/Bragg_Thesis.pdf" > (undergraduate thesis) </a> </li><br>

				<li> <i> Improving QoS in BitTorrent-like VoD Systems</i><br>
				Y. Yang, A. Chow, L. Golubchik, and <b>D. Bragg</b>. 
				IEEE INFOCOM 2010. <a href="papers/Imrpoving_QoS_in_BitTorrent-like_VoD_Systems.pdf" > (paper) </a> </li>

			</ul>
		</div>
		
		<!--
		<div class = "subtitle"> TEACHING </div>
		<br>
		
		<div class = "majorTextBox">
			 <a dataquery="#Link14iu" href="http://courses.cs.washington.edu/courses/cse446/13sp/">CSE 446: Machine Learning, 2013</a> - undergraduate Machine Learning course
		</div>
		-->


		<div class="centering">
		<h2 style="font-size: 16pt; font-weight: bold;"> CONTACT DANIELLE </h2>
		<a href="mailto:danielle.bragg@microsoft.com" title="Email me!"> Click to email me. </a>
		</div>
		<br>
	</div>

</body>
</html>
