<!DOCTYPE html>
<head>
	<meta name="author" content="Danielle Bragg" />
	<meta name="description" content="Danielle Bragg's website - Senior Researcher at Microsoft Research with interests in Human Computer Interaction, Accessibility, and Machine Learning" />
	<title>Danielle Bragg - Home </title>
	<meta name="keywords" content="Danielle Bragg, Computer Science, Human Computer Interaction, HCI, Machine Learning, ML, accessibility, Microsoft, Microsoft Research, MSR, University of Washington, UW" />
	<!--<meta name = "viewport" content = "user-scalable = yes">-->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<link rel="shortcut icon" href="images/light_transparent.png">

	<link rel="stylesheet" type="text/css" href="css/custom.css"/>

    <link href="libraries/bootstrap.min.css" rel="stylesheet">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"> </script>
	
	<script type='text/javascript'>

		
		$(document).ready(function(){
			
			$(".topnav a").click(function(){
			console.log('done!');
			$('html, body').animate({
				scrollTop: $(this.hash).offset().top - $(".topnav").height()}, 'slow');
			
			});
			
		});
	
	</script>
	
	
</head>

<body>

	<header class="intro" style="box-sizing: border-box;">

    </header>
	
	
	<div class="container">
	

		<div class = "content">

			  <div class="intro" style="overflow: hidden;">
				<img src="images/dbragg_headshot_tiny.png" style="width: 150px; float: left; margin-right: 30px; margin-top: 5px;">
				<br>
				<h1 style="font-size: 55px; font-weight: bold; margin-top: 0;">Danielle Bragg</h1>
				<p class="intro-text" style="margin-top: 0;">
				  
				  Senior Researcher<br>
				  Microsoft Research<br>
				  dbragg [at] post.harvard.edu
				</p>
			  </div>
			  

		<br>
		<!--<h2 class="subtitle" id="about"> ABOUT </h2>-->
		
		<div class = "majorTextBox"> 
		
			<!--<div style="width:58%; margin-right:2%; display: inline-block;">-->
			<div style="display: inline-block;">

				<p>I am a Senior Researcher at Microsoft Research, in the <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" target="_blank" style="cursor: pointer;"> New England</a> and <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/" target="_blank" style="cursor: pointer;">New York</a> labs. My primary research interests lie in developing computational systems that expand access to information, enhance productivity, and improve quality of life. My work is highly interdisciplinary, combining <b>Human-Computer Interaction</b>, <b>Applied Machine Learning</b>, and <b>Accessibility</b>. I also prioritize seeing projects through to real-world releases. My work has been featured in the Microsoft Research <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/blog/tackling-sign-language-data-inequity/">Blog</a> and <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/blog/accessible-systems-for-sign-language-computation-with-dr-danielle-bragg/" target="_blank" style="cursor: pointer;">Podcast</a>, and in <a dataquery="#Link14iu" href="https://www.economist.com/science-and-technology/2021/03/04/the-race-to-teach-sign-language-to-computers" target="_blank" style="cursor: pointer;">The Economist</a>.</p>
					
				<p>Prior to joining Microsoft Research, I completed a PhD in Computer Science & Engineering at the University of Washington, and a BA in Applied Mathematics from Harvard University.</p>

				<p> If you are interested in working with me, please check for <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/opportunities/">openings in New York</a> and/or reach out to me directly.</p>
				
				<!--
				<p>
				My diverse past research projects have spanned recommender systems, educational tools, data visualization, computational biology, computer music, applied mathematics, and network protocols.
				</p>-->		
				
				
			</div>
			
			<!--
			<div style="width:36%; float:right;">
				<h3><b> EXPERIENCE </b> </h3>
				<table class="experience" style="width:100%">
					<tr> 
						<th style="width:20%" valign="top"> Current </th>
						<td> Senior Researcher, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr> 
						<th style="width:20%" valign="top"> 2018-2020 </th>
						<td> Postdoc, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr> 
						<th style="width:20%" valign="top"> 2013-2018 </th>
						<td> PhD Student, <a dataquery="#Link14iu" href="http://www.cs.washington.edu/" target="_blank" style="cursor: pointer;">University of Washington</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2015, 2016 </th>
						<td> Intern, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2014 </th>
						<td> Intern, <a dataquery="#Link14iu" href="http://www.bing.com/">Microsoft Bing</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2011-2012 </th>
						<td> PhD Student, <a dataquery="#Link14iu" href="http://www.cs.princeton.edu/" targt="_blank" style="cursor: pointer;"> Princeton </a></td>
					</tr>
					<tr>
						<th valign="top"> 2010-2011 </th>
						<td> Research Assistant, <a dataquery="#Link14iu" href="http://www.cs.seas.gwu.edu/networking-and-mobile-computing"> George Washington University</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2010 </th>
						<td> AB, Applied Mathematics, <i> cum laude</i>, <a dataquery="#Link14iu" href="http://www.seas.harvard.edu/programs/applied-mathematics"> Harvard </a> </td>
					</tr>
				</table>
				
			</div>
		</div>
		-->
		
<!--
		<h2 class = "subtitle" id="research"> RESEARCH </h2> <br>
		
		<div class="majorTextBox">
		
			I build systems that improve access to information by leveraging modern computing capabilities in innovative ways. My work combines human-computer interaction (HCI) with applied machine learning, in collaboration with interdisciplinary colleagues from computer vision, machine learning, graphics, linguistics, typography, and the social sciences. My work employs both quantitative and qualitative methods, using data collected through crowdsourcing platforms to iterate on designs, explore solution spaces, and solve data scarcity problems. I partner with target users to incorporate input and build community trust, and am committed to publicly releasing projects so those users can benefit. Below is a list of some projects that I have led.
			
		</div>
		<br>

		<div class = "majorTextBox">
		
			<img class="projectImage_original" alt="Screen shot of ASL Wiki interface, showing a signer at the left and an article text on the right" src="images/algae_screenshot-1536x723.png" ></img>
				<h3 class = "subsubtitle"> ASL STEM Wiki - </h3>
				To help advance sign language modeling, we created ASL STEM Wiki - the first continuous signing dataset focused on Science, Technology, Engineering, and Math (STEM). The corpus contains 254 Wikipedia articles on STEM topics in English, interpreted into 300 hours of American Sign Language (ASL). In addition to its size and topic, unlike many prior datasets, it contains videos of professional signers (including many Deaf interpreters), was collected with consent under IRB approval, and Deaf team members were involved throughout. <div style="line-height:50%"><br></div>
				
				More: <a href="https://www.microsoft.com/en-us/research/project/asl-stem-wiki/">project page</a>, <a href="https://community.aslgames.org/wiki/">wiki</a>, paper below
		</div>
		

		<div class = "majorTextBox">
		
			<img class="projectImage_original" alt="Screen shot of ASL Sea Battle app, showing a signer at the top and a grid of tiles below" src="images/ASLCitizen_grid_blurred-1024x1024.png" ></img>
				<h3 class = "subsubtitle"> ASL Citizen - </h3>
				Lack of data is the biggest barrier to developing intelligent sign language systems. To help advance research, we release ASL Citizen, the first crowdsourced isolated sign language dataset. ASL Citizen is about 4x larger than prior single-sign datasets and samples everyday signers and environments. It is the only dataset of its scale to be collected with Deaf community involvement, consent, and compensation. Alongside the data, we release a searchable dictionary view of the dataset and baseline models with code. Our dataset brings state-of-the-art single-sign recognition from about 30% accuracy in prior work to 63% accuracy, under more challenging test conditions. <div style="line-height:50%"><br></div>
				
				More: <a href="https://www.microsoft.com/en-us/research/project/asl-citizen/">project page</a>, <a href="https://community.aslgames.org/explore/">dictionary</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of ASL Sea Battle app, showing a signer at the top and a grid of tiles below" src="images/bs_label_confirm_crop.png" ></img>
				<h3 class = "subsubtitle"> ASL SEA BATTLE - </h3>
				Lack of data is the biggest barrier to developing intelligent sign language systems. To collect larger, more representative AI datasets, we propose a sign language game. Our game is a variant of the traditional battleships game. In our ASL version, grid squares are labelled with signs, and players guess by recording a sign, thereby contributing to the dataset. The opponent views the video and taps the matching square to reveal a response, thereby providing a label. The game also provides a fun and educational sign language resource. <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/ASL_sea_battle_demo.mp4">recorded demo</a>, <a href="presentations/ASL Sea Battle.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of a signer with a tiger head avatar replacing their head" src="images/tiger pic signing crop.png" ></img>
				<h3 class = "subsubtitle"> SIGN LANGUAGE ML PRIVACY - </h3>
				Sign language data is intrinsically personal -- typically consisting of videos of people signing, which capture the face, body, and surroundings. We propose applying filters to these videos to help address privacy concerns, boost dataset participation, and thereby potentially boost ML model performance as well. We experimented with various filters applied to the signer and surroundings, and ran both user studies and computer vision experiments to validate this idea. <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/exploring collection.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Diagram of a wearable device, which consists of a baseball hat with an attached 3D-printed piece hosting a speaker/microphone, smart glasses, and camera with fisheye lens" src="images/chat in the hat.png" ></img>
				<h3 class = "subsubtitle"> CHAT IN THE HAT - </h3>
				Virtual remote interpreting (VRI) makes sign language interpretations available through a video call. VRI requires the signer to hold a smartphone (constraining signing and multitasking), or to place the video call device in a fixed location (constraining mobility). We prototyped a wearable device to free the signer's hands and enable mobility while using VRI. The device consists of a baseball hat with a speaker/mic (English input/output), mounted fisheye camera (ASL input), and smart glasses displaying the interpreter (ASL output).  <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/Chat in the hat.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="The word livefonts in several scripts, including an animated script" src="images/livefonts_crop.gif" ></img>
				<h3 class = "subsubtitle"> SMARTFONTS - </h3>
				By redesigning English letterforms, smartfonts and livefonts challenge our assumption that we should read traditional letterforms on modern screens. Personal devices enable this challenge, by allowing users to adopt new letterforms without language reform or mass adoption. While smartfonts leverage color, shape, and spacing, livefonts add animation to the design space. Potential benefits include increased legibility, privacy, aesthetics, and fun. Try it out by downloading <a href="images/Visibraille.ttf">a smartfont</a> and uploading it to Chrome's <a href="https://chrome.google.com/webstore/detail/font-changer/obgkjikcnonokgaiablbenkgjcdbknna?hl=en">Font Changer</a> extension, to render all your web content in a smartfont!<div style="line-height:50%"><br></div>
				
				More: <a href="smartfonts.html">interactive demo</a>, <a href="Smartfonts.mp4">smartfonts presentation</a>, <a href="presentations/Livefonts.mp4">livefonts presentation</a>, smartfonts paper below, livefonts paper below
		</div>
		
		
		<div class = "majorTextBox">
		
			<img class="projectImage_original" alt="an animated ASL character for the sign UNDERSTAND" src="images/understand.gif" style="padding-right: 35px; padding-left: 35px;"></img>
				<div class = "subsubtitle"> ANIMATED SI5S - </div>
				While ASL character/writing systems offer many potential benefits, none have been widely adopted. Time and effort to learn them is one obstacle to adoption. All past ASL scripts have been stationary (non-animated), which makes depicting movement difficult. We capitalize on modern animation capabilities, proposing the first animated ASL character system prototype. By representing movement iconically, our system reduces the need to memorize symbolic representations of complex movements.<div style="line-height:50%"><br></div>
				
				More: <a href="animated_si5s.html">interactive demo</a>, paper below
		</div>
		

		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Workflow of ASL-Search" src="images/asl-search_interface.png"></img>
			<h3 class = "subsubtitle"> ASL-SEARCH -  </h3>
			ASL-Search is an ASL dictionary that lets students look up the English meanings of signs. Looking up the meaning of a sign is difficult because a sign is a 3D movement not easily described with written words. Our dictionary lets users enter a set of features including hand shape, location, and movement, to describe a sign and look it up. The dictionary learns from the features that previous users enter to improve results for future users.<div style="line-height:50%"><br></div>
				
				More: <a dataquery="#Link14iu" href="https://asltoenglish.org" target="_blank" style="cursor: pointer;">https://asltoenglish.org</a> (beta release), paper below
		</div>
		
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Workflow of ASL-Search" src="images/asl-flash_descriptive.png" width="100px" style="float:left; outline: black solid thin; margin-right:10px"></img>
			
			<h3 class = "subsubtitle"> ASL-FLASH - </h3>
			
			ASL-Flash is a site that both helps people learn American Sign Language and provides featural descriptions of signs. The site provides "flashcards" of signs, showing visitors videos of signs and quizzing them on the English meanings and compositional features (e.g., handshape and hand location). The data that users provide helps us build the ASL-Search dictionary. Check it out at <a dataquery="#Link14iu" href="http://www.aslflash.org" target="_blank" style="cursor: pointer;"> www.aslflash.org </a> and learn some signs!<div style="line-height:50%"><br></div>
				
				More: <a dataquery="#Link14iu" href="https://aslflash.org" target="_blank" style="cursor: pointer;">https://aslflash.org</a>, paper below
		</div>
		
		
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of the study, with the words 'Press play when you are ready' and a large play button" src="images/listening_rates_play.png"></img>
			

			<h3 class = "subsubtitle"> LISTENING RATES - </h3>
		
			We provide the first inclusive, large-scale studies on human listening rates. As conversational agents and digital assistants become increasingly pervasive, understanding their synthetic speech becomes increasingly important. Our studies presented volunteer participants with a series of challenging listening questions in order to learn more about their own listening rates. Our results inform synthetic speech rate optimization and future inclusive crowdsourced studies.
		<div style="line-height:50%"><br></div>
				
				More: initial study paper below, expanded study paper below
		</div>
		

		</div>
		
	-->
		



	
		<h2 class = "subtitle" id="honors"> HONORS </h2>
		
		<div class = "majorTextBox">
			<ul>
				<li> <a href="https://programs.sigchi.org/chi/2025/awards/best-papers">Honorable Mention Award, CHI</a> 2025 </li>
				<li> <a href="https://cscw.acm.org/2022/awards/">Impact Award, CSCW</a> 2022 </li>
				<li> <a href="https://assets21.sigaccess.org/program.html">Best Paper Nominee, ASSETS</a> 2021 </li>
				<li> <a href="https://programs.sigchi.org/chi/2021/awards">Honorable Mention Award, CHI</a> 2021 </li>
				<li> <a href="https://www.sigaccess.org/category/news/awards/best-paper/">Best Paper Award, ASSETS</a> 2019 </li>
				<li> <a href="https://blogs.microsoft.com/accessibility/ability-summit-2020/">Microsoft Ability Summit</a>, Innovation Award Winner</a> 2019 </li>
				<li> <a href="https://blogs.microsoft.com/newengland/2018/06/29/new-england-machine-learning-accessibility/"> New England Machine Learning Accessibility Hackathon, 1st Place</a> 2018 </li>
				<li> <a href="http://www.aspirations.org/2017-ncwit-collegiate-award-recipients"> National Center for Women & Information Technology (NCWIT) Collegiate Award</a> 2017 </li>
				<li> <a href="http://expd.uw.edu/expo/scholarships/JMRS">Judith M. Runstad Wells Fargo Women's Roundtable Scholarship</a> 2017 </li>
				<li> <a href="https://disabilitystudies.washington.edu/Hahn_Lang_awards_2017">Harlan Hahn Endowment Fund Grant</a> 2017 </li>
				<li> <a href="http://www.microsoft.com/en-us/research/academic-program/womens-fellowship-program/">Microsoft Research Graduate Women's Scholarship</a> 2012</li>
			
			</ul>
		</div>
		
	
		
		<h2 class = "subtitle" id="publications"> SELECT PUBLICATIONS (<a dataquery="#Link14iu" href="http://scholar.google.com/citations?user=6fd-LUEAAAAJ&hl=en&oi=ao">Google Scholar</a>)</h2>
		
		<div class = "majorTextBox">
			<ul>

				<li> <i>Exploring Reduced Feature Sets for American Sign Language Dictionaries</i> <br> B. Kosa, A. Desai, A. Lu, R. Ladner, <b>D. Bragg</b> CHI 2025.  <span class="badge badge-award">Honorable Mention</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/Exploring Reduced Feature Sets for ASL DIctionaries.pdf">(paper)</a> </li><span style="display: block; height: 0.5em;"></span>

				<li> <i>Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review</i> <br> R. Pang, H. Schroeder, K. Smith, S. Barocas, Z. Xiao, E. Tseng, <b>D. Bragg</b> CHI 2025. <a href="papers/Understanding the LLM-ification of CHI.pdf">(paper)</a></i> <span style="display: block; height: 0.5em;"></span>

				<li> <i>EyeO: Autocalibrating Gaze Output with Gaze Input for Gaze Typing</i><br> A. Sara, J. Alber, C. Zhang, A. Paradiso, <b>D. Bragg</b>, J. Langford. CHI LBW 2025. <a href="papers/EyeO - Autocalibrating Gaze Output with Gaze Input.pdf">(paper)</a><span style="display: block; height: 0.5em;"></span>

				<li> <i>ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles</i> <br> K. Yin, C. Singh, F. Minakov, V. Milan, H. Daum&#233 III, C. Zhang, A. Lu, <b>D. Bragg</b>. EMNLP 2024. <span class="badge badge-award">Multi-part Release</span> <a href="https://arxiv.org/pdf/2411.05783">(paper)</a>, <a href="https://aslgames.azurewebsites.net/wiki">(bilingual Wiki)</a>, <a href="https://www.microsoft.com/en-us/research/project/asl-stem-wiki/">(project page)</a>, <a href="https://www.microsoft.com/en-us/download/details.aspx?id=106253">(dataset)</a>, <a href="https://github.com/microsoft/ASL-STEM-Wiki">(repo)</a> 
					<span style="display: block; height: 0.5em;"></span>

				<li> <i>Studying and Mitigating Biases in Sign Language Understanding Models</i> <br> K. Atwell, <b>D. Bragg</b>, M. Alikhani. EMNLP 2024. <a href="https://arxiv.org/pdf/2410.05206">(paper)</a><span style="display: block; height: 0.5em;"></span>

				<li> <i>The Semantic Reader Project</i><br> K. Lo et al. Communications of the ACM (CACM) 2024. <a href="https://arxiv.org/pdf/2303.14334.pdf">(pre-print)</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3659096">(article)</a><span style="display: block; height: 0.5em;"></span>

				<li> <i>ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition</i> <br> A. Desai, L. Berger, F. Minakov, V. Milan, C. Singh, K. Pumphrey, R. Ladner, H. Daum&#233 III, A. Lu, N. Caselli, <b>D. Bragg</b>. NeurIPS 2023. <span class="badge badge-award">Multi-part Release</span> <a href="https://arxiv.org/abs/2304.05934">(paper)</a>, <a href="https://community.aslgames.org/explore/">(community dictionary)</a>, <a href="https://www.microsoft.com/en-us/research/project/asl-citizen/">(project page)</a>, <a href="https://www.microsoft.com/en-us/download/details.aspx?id=105253">(dataset)</a>, <a href="https://github.com/microsoft/ASL-citizen-code">(repo)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>U.S. Deaf Community Perspectives on Automatic Sign Language Translation</i><br> N. Tran, R. Ladner, <b>D. Bragg</b>. ASSETS 2023. <a href="papers/Automatic_Sign_Language_Translation_Perspectives.pdf">(paper)</a><span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Mixed Abilities and Varied Experiences: A Group Autoethnography of a Virtual Summer Internship</i> <br>
					K. Mack, M. Das, D. Jain, <b>D. Bragg</b>, J. Tang, A. Begel, E. Benetau, J. Davis, A. Glasser, J. Park, V. Potluri. 
					Communications of the ACM 2023. <a href="https://dl.acm.org/doi/pdf/10.1145/3604622">(article)</a><span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Tech Worker Perspectives on Considering the Interpersonal Impications of Communication Technologies</i> <br> E. Maris, K. Wagman, R. Bergmann, <b>D. Bragg</b>. GROUP 2023. <a href="papers/Tech Worker Perspectives.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Exploring Team-Sourced Hyperlinks to Address Navigation Challenges for Low-Vision Readers of Scientific Papers</i> <br> S. Park, J. Bragg, M. Chang, K. Larson, <b>D. Bragg</b>. CSCW 2022. <a href="papers/Ocean.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Exploring Collection of Sign Language Videos through Crowdsourcing</i> <br> <b>D. Bragg</b>, A. Glasser, F. Minakov, N. Caselli, W. Thies. CSCW 2022. <span class="badge badge-award">Impact Award</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/Exploring_Collection_of_Sign_Language_Videos_through_Crowdsourcing.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				
				<li> <i>ASL Wiki: An Exploratory Interface for Crowdsourcing ASL Translations</i> <br> A. Glasser, F. Minakov, <b>D. Bragg</b>. ASSETS 2022. <a href="papers/ASL_Wiki.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Mixed Abilities and Varied Experiences: A Group Autoethnography of a Virtual Summer Internship</i> <br>
				K. Mack, M. Das, D. Jain, <b>D. Bragg</b>, J. Tang, A. Begel, E. Benetau, J. Davis, A. Glasser, J. Park, V. Potluri. 
				ASSETS 2021. <span class="badge badge-award">Best Paper Nominee</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/08/mack_etal_mixed_abilities_autoethno.pdf">(paper)</a> 
				
				
				<li> <i>Expanding a Large Inclusive Study of Human Listening Rates</i> <br>
				<b>D. Bragg</b>, K. Reinecke, R. Ladner. 
				TACCESS 2021. <a href="papers/Expanding_a_Large_Inclusive_Study.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>The FATE Landscape of Sign Language AI Datasets: An Interdisciplinary Perspective</i> <br>
				<b>D. Bragg</b>, N. Caselli, J. Hochgesang, M. Huenerfauth, L. Katz-Hernandez, O. Koller, R. Kushalnagar, C. Vogler, R. Ladner. 
				TACCESS 2021. <a href="papers/The_FATE_Landscape.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				
				<li> <i>ASL Sea Battle: Gamifying Sign Language Data Collection</i> <br>
				<b>D. Bragg</b>, N. Caselli, J. W. Gallagher, M. Goldberg, C. Oka, W. Thies. CHI 2021. <span class="badge badge-award">Honorable Mention</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/ASL_Sea_Battle.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>The Promise and Peril of Parallel Chat in Video Meetings for Work</i> <br>
				A. Sarkar, S. Rintel, D. Borowiec, R. Bergmann, S. Gillett, <b>D. Bragg</b>, N. Baym, A. Sellen. CHI LBW 2021. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/02/2021_CHI_LBW___parallel_chat___author_own_version_custom_footer.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Designing an Online Infrastructure for Collecting AI Data From People With Disabilities</i> <br>
				J. Park, <b>D. Bragg</b>, E. Kamar., M.R. Morris. FAccT 2021. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/01/Inclusive_AI_Datasets_FINAL.pdf">(paper)</a><span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Exploring Collection of Sign Language Datasets: Privacy, Participation, and Model Performance</i> <br>
				<b>D. Bragg</b>, O. Koller, N. Caselli, W. Thies. 
				ASSETS 2020. <a href="papers/Exploring_Collection_of_Sign_Language_Datasets.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Chat in the Hat: A Portable Interpreter for Sign Language Users</i> <br> 
				L. Berke, W. Thies, <b>D. Bragg</b>.
				ASSETS 2020.  <a href="papers/Chat_in_the_Hat.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Social App Accessibility for Deaf Signers</i> <br>
				K. Mack, <b>D. Bragg</b>, M.R. Morris, M. Boss, I. Albi, A. Monroy-Hernandez.
				CSCW 2020. <a href="papers/Social_App_Accessibility_for_Deaf_Signers.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
								
				<li> <i>Sign Language Interfaces: Discussing the Field's Biggest Challenges</i> <br>
				<b>D. Bragg</b>, M.R. Morris, C. Vogler, R. Kushalnagar, M. Huenerfauth, H. Kacorri.
				CHI 2020. <a href="papers/Sign_Language_Interfaces.pdf">(extended abstract)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective</i> <br>
				<b>D. Bragg</b>, O. Koller, M. Bellard, L. Berke, P. Boudrealt, A. Braffort, N. Caselli, M. Huenerfauth, H. Kacorri, T. Verhoef, C. Vogler, M.R. Morris. 
				ASSETS 2019. <span class="badge badge-award">Best Paper</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/Sign_Language_Workshop.pdf">(paper)</a> <a href="https://arxiv.org/pdf/1908.08597.pdf">(pre-print)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i> Designing an Animated Character System for American Sign Language</i> <br>
				<b>D. Bragg</b>, R. Kushalnagar, R. Ladner. 
				ASSETS 2018. <a href="papers/Animated_Si5s.pdf">(paper)</a> <small> <b> <--- NOTE: Open with Adobe Reader to see animations <i>in the PDF itself!</i> </b> </small>  </li> <span style="display: block; height: 0.5em;"></span>
									
				<li> <i> A Large Inclusive Study of Human Listening Rates</i> <br>
				<b>D. Bragg</b>, C. Bennett, K. Reinecke, R. Ladner.
				CHI 2018. <a href="papers/listening_rates.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Designing and Evaluating Livefonts</i><br>
				 <b>D. Bragg</b>, S. Azenkot, K. Larson, A. Bessemans, A. Kalai.
				 UIST 2017. <a href="papers/livefonts.pdf">(paper)</a> <small> <b> <--- NOTE: Open with Adobe Reader to see animations <i>in the PDF itself!</i> </b> </small>  <span style="display: block; height: 0.5em;"></span>
				
				<li> <i>Reading and Learning Smartfonts</i> <br>
				 <b>D. Bragg</b>, S. Azenkot, A. Kalai. 
				 UIST 2016. <a href="papers/Reading_and_Learning_Smartfonts.pdf">(paper)</a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i> A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users</i><br>
				<b>D. Bragg</b>, N. Huynh, R. Ladner. 
				ASSETS 2016. <a href="papers/Personalizable_Mobile_Sound_Detector.pdf"> (paper) </a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i> A User-Powered American Sign Language Dictionary</i><br>
				<b>D. Bragg</b>, K. Rector, R. Ladner. 
				CSCW 2015. <a href="papers/A_User-Powered_American_Sign_Language_Dictionary.pdf"> (paper) </a> <span style="display: block; height: 0.5em;"></span>
			
				<li> <i> Synchronous Data Flow Modeling for DMIs</i><br>
				<b>D. Bragg</b>. 
				NIME 2013. <a href="papers/Synchronous_Data_Flow_Modeling.pdf" > (paper) </a> <span style="display: block; height: 0.5em;"></span>

				<!--<li> Bragg, D., M. Yun, H. Bragg, H-A. Choi. “Intelligent Transmission of Patient Sensor Data in Wireless Hospital Networks.” Proc. AMIA (American Medical Informatics Association) Symposium. Chicago, Illinois. Nov 2012. <a href="papers/Intelligent_Transmission.pdf" > (paper) </a> </li>-->
				
				<!--<li> Bragg, D., M. Yun, H. Bragg, and H.A. Choi. “Game Theoretical Approach to Scheduling Transmission of Data for Hospital Patients.” Tel Aviv International Workshop on Game Theory. Tel Aviv, Israel. June 2011. (poster) </li>-->
				
				<li> <i> Battle Event Detection Using Sensor Networks and Distributed Query Processing</i><br>
				M. Yun, <b>D. Bragg</b>, A. Arora, and H.A. Choi.
				IEEE INFOCOM 2011. <a href="papers/Battle_Event_Detection.pdf" > (paper) </a> <span style="display: block; height: 0.5em;"></span>
				
				<li> <i> On Data Transmission Scheduling considering Switching Penalty in Mobile Sensor Networks</i><br>
				Y. Zhou, <b>D. Bragg</b>, M. Yun, and H.A. Choi.
				IEEE INFOCOM 2011. <a href="papers/On_Data_Transmission_Scheduling.pdf" > (paper) </a> <span style="display: block; height: 0.5em;"></span>
				
				<!--
				<li> <i> Quantification and Display of Emotions in Music</i><br>
				<b>D. Bragg</b>. 
				Honors Senior Thesis, Harvard University Department of Applied Mathematics, 2010. <a href="papers/Bragg_Thesis.pdf" > (undergraduate thesis) </a> <span style="display: block; height: 0.5em;"></span>
				-->

				<li> <i> Improving QoS in BitTorrent-like VoD Systems</i><br>
				Y. Yang, A. Chow, L. Golubchik, and <b>D. Bragg</b>. 
				IEEE INFOCOM 2010. <a href="papers/Imrpoving_QoS_in_BitTorrent-like_VoD_Systems.pdf" > (paper) </a> </li>

			</ul>
		</div>


		<br>
	</div>

</body>
</html>
