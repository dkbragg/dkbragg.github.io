<!DOCTYPE html>
<head>
	<meta name="author" content="Danielle Bragg" />
	<meta name="description" content="Danielle Bragg's website - Senior Researcher at Microsoft Research with interests in Human Computer Interaction, Accessibility, and Machine Learning" />
	<title>Danielle Bragg - Home </title>
	<meta name="keywords" content="Danielle Bragg, Computer Science, Human Computer Interaction, HCI, Machine Learning, ML, accessibility, Microsoft, Microsoft Research, MSR, University of Washington, UW" />
	<!--<meta name = "viewport" content = "user-scalable = yes">-->
	<meta name="viewport" content="width=device-width, initial-scale=1">
	
	<link rel="shortcut icon" href="images/light_transparent.png">

	<link rel="stylesheet" type="text/css" href="css/custom.css"/>

    <link href="libraries/bootstrap.min.css" rel="stylesheet">

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"> </script>
	
	<script type='text/javascript'>

		
		$(document).ready(function(){
			
			$(".topnav a").click(function(){
			console.log('done!');
			$('html, body').animate({
				scrollTop: $(this.hash).offset().top - $(".topnav").height()}, 'slow');
			
			});
			
		});
	
	</script>
	
	
</head>

<body>


    <header class="intro" style="background-image: url('images/20160616_203445_tiny.png'); background-size: cover;">


			<div class="topnav">
			  <div>Danielle Bragg</div>
			  <a href="#about">About</a>
			  <a href="#research">Research</a>
			  <a href="#honors">Honors</a>
			  <a href="#publications">Publications</a>
			</div>


		<div class="intro strokeme">
			<div class="centering">
			 <img class="img-circle" src="images/dbragg_headshot_tiny.png" style="width:200px; border: 6px solid white;"> 
				<h1 class="" style="font-size:55px; font-weight:bold; color:white;">Danielle Bragg</h1>
			  <p class="intro-text" style="">Senior Researcher <br>
				Microsoft Research <br>
				danielle.bragg [at] microsoft.com 
			  </p>
			</div>

        </div>

    </header>
	
	
	<div class="container">
		
		<div class = "content">
		<br>
		<h2 class="subtitle" id="about"> ABOUT ME </h2> <br>
		
		<div class = "majorTextBox"> 
		
			<div style="width:58%; margin-right:2%; display: inline-block;">
				<p>I am a Senior Researcher at Microsoft Research, in the <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" target="_blank" style="cursor: pointer;"> New England</a> and <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/" target="_blank" style="cursor: pointer;">New York</a> labs. </p>
					
				<p>My research focuses on developing systems that expand acces to information, in particular for people with disabilities. 
				My work is interdisciplinary, combining <b>Human-Computer Interaction</b>, <b>Applied Machine Learning</b>, and <b>Accessibility</b>. I completed my PhD in Computer Science at the <a dataquery="#Link14iu" href="http://www.cs.washington.edu/" target="_blank" style="cursor: pointer;">University of Washington</a> advised by <a dataquery="#Link14iu" href="http://www.cs.washington.edu/people/faculty/ladner/" target="_blank" style="cursor: pointer;"> Richard Ladner</a>, and hold an AB in Applied Mathematics from <a dataquery="#Link14iu" href="http://www.seas.harvard.edu/programs/applied-mathematics"> Harvard</a>. My work has been featured in the <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/blog/accessible-systems-for-sign-language-computation-with-dr-danielle-bragg/" target="_blank" style="cursor: pointer;">Microsoft Research Podcast</a> and in <a dataquery="#Link14iu" href="https://www.economist.com/science-and-technology/2021/03/04/the-race-to-teach-sign-language-to-computers" target="_blank" style="cursor: pointer;">The Economist</a>.</p>

				<!--
				<p>My recent work has focused largely on building systems to better support sign language users. Building these systems requires tackling a variety of challenges in data collection, modeling, visual interface design, social interactions, and beyond. To learn more, check out this <a dataquery="#Link14iu" href="https://www.microsoft.com/en-us/research/blog/accessible-systems-for-sign-language-computation-with-dr-danielle-bragg/" target="_blank" style="cursor: pointer;">podcast</a>, where I discuss challenges facing the field of sign language computation. My work has also been featured in <a dataquery="#Link14iu" href="https://www.economist.com/science-and-technology/2021/03/04/the-race-to-teach-sign-language-to-computers" target="_blank" style="cursor: pointer;">The Economist</a>.
				</p>
				
				<p>
				My diverse past research projects have spanned recommender systems, educational tools, data visualization, computational biology, computer music, applied mathematics, and network protocols.
				</p>-->				
				
				
			</div>
			
			<div style="width:36%; float:right;">
				<h3><b> EXPERIENCE </b> </h3>
				<table class="experience" style="width:100%">
					<tr> 
						<th style="width:20%" valign="top"> Current </th>
						<td> Senior Researcher, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr> 
						<th style="width:20%" valign="top"> 2018-2020 </th>
						<td> Postdoc, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr> 
						<th style="width:20%" valign="top"> 2013-2018 </th>
						<td> PhD Student, <a dataquery="#Link14iu" href="http://www.cs.washington.edu/" target="_blank" style="cursor: pointer;">University of Washington</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2015, 2016 </th>
						<td> Intern, <a dataquery="#Link14iu" href="http://www.microsoft.com/en-us/research/lab/microsoft-research-new-england/" targt="_blank" style="cursor: pointer;">Microsoft Research</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2014 </th>
						<td> Intern, <a dataquery="#Link14iu" href="http://www.bing.com/">Microsoft Bing</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2011-2012 </th>
						<td> PhD Student, <a dataquery="#Link14iu" href="http://www.cs.princeton.edu/" targt="_blank" style="cursor: pointer;"> Princeton </a></td>
					</tr>
					<tr>
						<th valign="top"> 2010-2011 </th>
						<td> Research Assistant, <a dataquery="#Link14iu" href="http://www.cs.seas.gwu.edu/networking-and-mobile-computing"> George Washington University</a> </td>
					</tr>
					<tr>
						<th valign="top"> 2010 </th>
						<td> AB, Applied Mathematics, <i> cum laude</i>, <a dataquery="#Link14iu" href="http://www.seas.harvard.edu/programs/applied-mathematics"> Harvard </a> </td>
					</tr>
				</table>
				
			</div>
		</div>
		

		<h2 class = "subtitle" id="research"> RESEARCH </h2> <br>
		
		<div class="majorTextBox">
		
			I build systems that improve access to information by leveraging modern computing capabilities in innovative ways. My work combines human-computer interaction (HCI) with applied machine learning, in collaboration with interdisciplinary colleagues from computer vision, machine learning, graphics, linguistics, typography, and the social sciences. My work employs both quantitative and qualitative methods, using data collected through crowdsourcing platforms to iterate on designs, explore solution spaces, and solve data scarcity problems. I partner with target users to incorporate input and build community trust, and am committed to publicly releasing projects so those users can benefit. Below is a list of some projects that I have led.
			<!--My research seeks to make information more available, in particular to sign language users and low-vision readers. As interactive technologies become increasingly rich, multi-modal, and pervasive, ensuring access becomes increasingly important and challenging. At the same time, new technologies and machine learning techniques offer new capabilities that can improve accessibility, and introduce opportunities for innovation. My work seeks to leverage these opportunities to help address barriers.-->
			
		</div>
		<br>


			

		<div class = "majorTextBox">
		
			<img class="projectImage_original" alt="Screen shot of ASL Wiki interface, showing a signer at the left and an article text on the right" src="images/algae_screenshot-1536x723.png" ></img>
				<h3 class = "subsubtitle"> ASL STEM Wiki - </h3>
				To help advance sign language modeling, we created ASL STEM Wiki - the first continuous signing dataset focused on Science, Technology, Engineering, and Math (STEM). The corpus contains 254 Wikipedia articles on STEM topics in English, interpreted into 300 hours of American Sign Language (ASL). In addition to its size and topic, unlike many prior datasets, it contains videos of professional signers (including many Deaf interpreters), was collected with consent under IRB approval, and Deaf team members were involved throughout. <div style="line-height:50%"><br></div>
				
				More: <a href="https://www.microsoft.com/en-us/research/project/asl-stem-wiki/">project page</a>, <a href="https://community.aslgames.org/wiki/">wiki</a>, paper below
		</div>
		

		<div class = "majorTextBox">
		
			<img class="projectImage_original" alt="Screen shot of ASL Sea Battle app, showing a signer at the top and a grid of tiles below" src="images/ASLCitizen_grid_blurred-1024x1024.png" ></img>
				<h3 class = "subsubtitle"> ASL Citizen - </h3>
				Lack of data is the biggest barrier to developing intelligent sign language systems. To help advance research, we release ASL Citizen, the first crowdsourced isolated sign language dataset. ASL Citizen is about 4x larger than prior single-sign datasets and samples everyday signers and environments. It is the only dataset of its scale to be collected with Deaf community involvement, consent, and compensation. Alongside the data, we release a searchable dictionary view of the dataset and baseline models with code. Our dataset brings state-of-the-art single-sign recognition from about 30% accuracy in prior work to 63% accuracy, under more challenging test conditions. <div style="line-height:50%"><br></div>
				
				More: <a href="https://www.microsoft.com/en-us/research/project/asl-citizen/">project page</a>, <a href="https://community.aslgames.org/explore/">dictionary</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of ASL Sea Battle app, showing a signer at the top and a grid of tiles below" src="images/bs_label_confirm_crop.png" ></img>
				<h3 class = "subsubtitle"> ASL SEA BATTLE - </h3>
				Lack of data is the biggest barrier to developing intelligent sign language systems. To collect larger, more representative AI datasets, we propose a sign language game. Our game is a variant of the traditional battleships game. In our ASL version, grid squares are labelled with signs, and players guess by recording a sign, thereby contributing to the dataset. The opponent views the video and taps the matching square to reveal a response, thereby providing a label. The game also provides a fun and educational sign language resource. <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/ASL_sea_battle_demo.mp4">recorded demo</a>, <a href="presentations/ASL Sea Battle.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of a signer with a tiger head avatar replacing their head" src="images/tiger pic signing crop.png" ></img>
				<h3 class = "subsubtitle"> SIGN LANGUAGE ML PRIVACY - </h3>
				Sign language data is intrinsically personal -- typically consisting of videos of people signing, which capture the face, body, and surroundings. We propose applying filters to these videos to help address privacy concerns, boost dataset participation, and thereby potentially boost ML model performance as well. We experimented with various filters applied to the signer and surroundings, and ran both user studies and computer vision experiments to validate this idea. <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/exploring collection.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Diagram of a wearable device, which consists of a baseball hat with an attached 3D-printed piece hosting a speaker/microphone, smart glasses, and camera with fisheye lens" src="images/chat in the hat.png" ></img>
				<h3 class = "subsubtitle"> CHAT IN THE HAT - </h3>
				Virtual remote interpreting (VRI) makes sign language interpretations available through a video call. VRI requires the signer to hold a smartphone (constraining signing and multitasking), or to place the video call device in a fixed location (constraining mobility). We prototyped a wearable device to free the signer's hands and enable mobility while using VRI. The device consists of a baseball hat with a speaker/mic (English input/output), mounted fisheye camera (ASL input), and smart glasses displaying the interpreter (ASL output).  <div style="line-height:50%"><br></div>
				
				More: <a href="presentations/Chat in the hat.mp4">presentation</a>, paper below
		</div>
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="The word livefonts in several scripts, including an animated script" src="images/livefonts_crop.gif" ></img>
				<h3 class = "subsubtitle"> SMARTFONTS - </h3>
				By redesigning English letterforms, smartfonts and livefonts challenge our assumption that we should read traditional letterforms on modern screens. Personal devices enable this challenge, by allowing users to adopt new letterforms without language reform or mass adoption. While smartfonts leverage color, shape, and spacing, livefonts add animation to the design space. Potential benefits include increased legibility, privacy, aesthetics, and fun. Try it out by downloading <a href="images/Visibraille.ttf">a smartfont</a> and uploading it to Chrome's <a href="https://chrome.google.com/webstore/detail/font-changer/obgkjikcnonokgaiablbenkgjcdbknna?hl=en">Font Changer</a> extension, to render all your web content in a smartfont!<div style="line-height:50%"><br></div>
				
				More: <a href="smartfonts.html">interactive demo</a>, <a href="Smartfonts.mp4">smartfonts presentation</a>, <a href="presentations/Livefonts.mp4">livefonts presentation</a>, smartfonts paper below, livefonts paper below
		</div>
		
		
		<div class = "majorTextBox">
		
			<!--<img class="projectImage_original" alt="The word livefonts in several scripts, including an animated script" src="images/livefonts_crop.gif" ></img>-->
		
			<img class="projectImage_original" alt="an animated ASL character for the sign UNDERSTAND" src="images/understand.gif" style="padding-right: 35px; padding-left: 35px;"></img>
				<div class = "subsubtitle"> ANIMATED SI5S - </div>
				While ASL character/writing systems offer many potential benefits, none have been widely adopted. Time and effort to learn them is one obstacle to adoption. All past ASL scripts have been stationary (non-animated), which makes depicting movement difficult. We capitalize on modern animation capabilities, proposing the first animated ASL character system prototype. By representing movement iconically, our system reduces the need to memorize symbolic representations of complex movements.<div style="line-height:50%"><br></div>
				
				More: <a href="animated_si5s.html">interactive demo</a>, paper below
		</div>
		

		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Workflow of ASL-Search" src="images/asl-search_interface.png"></img>
			<h3 class = "subsubtitle"> ASL-SEARCH -  </h3>
			ASL-Search is an ASL dictionary that lets students look up the English meanings of signs. Looking up the meaning of a sign is difficult because a sign is a 3D movement not easily described with written words. Our dictionary lets users enter a set of features including hand shape, location, and movement, to describe a sign and look it up. The dictionary learns from the features that previous users enter to improve results for future users.<div style="line-height:50%"><br></div>
				
				More: <a dataquery="#Link14iu" href="https://asltoenglish.org" target="_blank" style="cursor: pointer;">https://asltoenglish.org</a> (beta release), paper below
		</div>
		
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Workflow of ASL-Search" src="images/asl-flash_descriptive.png" width="100px" style="float:left; outline: black solid thin; margin-right:10px"></img>
			
			<h3 class = "subsubtitle"> ASL-FLASH - </h3>
			
			ASL-Flash is a site that both helps people learn American Sign Language and provides featural descriptions of signs. The site provides "flashcards" of signs, showing visitors videos of signs and quizzing them on the English meanings and compositional features (e.g., handshape and hand location). The data that users provide helps us build the ASL-Search dictionary. Check it out at <a dataquery="#Link14iu" href="http://www.aslflash.org" target="_blank" style="cursor: pointer;"> www.aslflash.org </a> and learn some signs!<div style="line-height:50%"><br></div>
				
				More: <a dataquery="#Link14iu" href="https://aslflash.org" target="_blank" style="cursor: pointer;">https://aslflash.org</a>, paper below
		</div>
		
		
		
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of the study, with the words 'Press play when you are ready' and a large play button" src="images/listening_rates_play.png"></img>
			

			<h3 class = "subsubtitle"> LISTENING RATES - </h3>
		
			We provide the first inclusive, large-scale studies on human listening rates. As conversational agents and digital assistants become increasingly pervasive, understanding their synthetic speech becomes increasingly important. Our studies presented volunteer participants with a series of challenging listening questions in order to learn more about their own listening rates. Our results inform synthetic speech rate optimization and future inclusive crowdsourced studies.
		<div style="line-height:50%"><br></div>
				
				More: initial study paper below, expanded study paper below
		</div>
		

		</div>
		
		
		<!--
		<div class = "majorTextBox">
			<img class="projectImage_original" alt="Screen shot of the sound detector app, listing sounds of interest to the user" src="images/detector_small.png"></img>
			

			<h3 class = "subsubtitle"> SOUND DETECTOR - </h3>
			
			The sound detector is a trainable app that alerts users to sounds of interest (e.g., a door knock, appliance running, or alarm ringing). Sounds provide important information, and non-auditory cues are not always available. In these situations, a sound detector can be useful to deaf or hard-of-hearing people. Our mobile app design provides personalized sound awareness through a ubiquitous device. The user records examples of sounds, and the app notifies the user when they occur.

		</div>
		-->
		<!--
		<h2 class = "subtitle"> PREVIOUS STUDENTS/INTERNS </div>
		<br>
		
		Larwan Berke (RIT/Gallaudet), Abraham Glasser, 
		-->
		
		
		<h2 class = "subtitle" id="honors"> HONORS </h2>
		
		<br>
		
		<div class = "majorTextBox">
			<ul>
				<li> <a href="https://cscw.acm.org/2022/awards/">Impact Award, CSCW</a> 2022 </li>
				<li> <a href="https://assets21.sigaccess.org/program.html">Best Paper Nominee, ASSETS</a> 2021 </li>
				<li> <a href="https://programs.sigchi.org/chi/2021/awards">Honorable Mention Award, CHI</a> 2021 </li>
				<li> <a href="https://www.sigaccess.org/category/news/awards/best-paper/">Best Paper Award, ASSETS</a> 2019 </li>
				<li> <a href="https://blogs.microsoft.com/accessibility/ability-summit-2020/">Microsoft Ability Summit</a>, Innovation Award Winner</a> 2019 </li>
				<li> <a href="https://blogs.microsoft.com/newengland/2018/06/29/new-england-machine-learning-accessibility/"> New England Machine Learning Accessibility Hackathon, 1st Place</a> 2018 </li>
				<li> <a href="http://www.aspirations.org/2017-ncwit-collegiate-award-recipients"> National Center for Women & Information Technology (NCWIT) Collegiate Award</a> 2017 </li>
				<li> <a href="http://expd.uw.edu/expo/scholarships/JMRS">Judith M. Runstad Wells Fargo Women's Roundtable Scholarship</a> 2017 </li>
				<li> <a href="https://disabilitystudies.washington.edu/Hahn_Lang_awards_2017">Harlan Hahn Endowment Fund Grant</a> 2017 </li>
				<li> <a href="http://www.microsoft.com/en-us/research/academic-program/womens-fellowship-program/">Microsoft Research Graduate Women's Scholarship</a> 2012</li>
			
			</ul>
		</div>
		
		
		<h2 class = "subtitle" id="publications"> SELECT PUBLICATIONS (<a dataquery="#Link14iu" href="http://scholar.google.com/citations?user=6fd-LUEAAAAJ&hl=en&oi=ao">Google Scholar</a>)</h2><br>
		
		<div class = "majorTextBox">
			<ul>

				<li> <i>ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles</i> <br> K. Yin, C. Singh, F. Minakov, V. Milan, H. Daum&#233 III, C. Zhang, A. Lu, <b>D. Bragg</b>. EMNLP 2024. <a href="https://arxiv.org/pdf/2411.05783">(paper)</a></li><br>

				<li> <i>Studying and Mitigating Biases in Sign Language Understanding Models</i> <br> K. Atwell, <b>D. Bragg</b>, M. Alikhani. EMNLP 2024. <a href="https://arxiv.org/pdf/2410.05206">(paper)</a></li><br>

				<li> <i>The Semantic Reader Project</i><br> K. Lo et al. Communications of the ACM (CACM) 2024. <a href="https://arxiv.org/pdf/2303.14334.pdf">(pre-print)</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3659096">(article)</a></li><br>

				<li> <i>ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition</i> <br> A. Desai, L. Berger, F. Minakov, V. Milan, C. Singh, K. Pumphrey, R. Ladner, H. Daum&#233 III, A. Lu, N. Caselli, <b>D. Bragg</b>. NeurIPS 2023. <a href="https://arxiv.org/abs/2304.05934">(paper)</a></li><br>
				
				<li> <i>U.S. Deaf Community Perspectives on Automatic Sign Language Translation</i><br> N. Tran, R. Ladner, <b>D. Bragg</b>. ASSETS 2023. <a href="papers/Automatic_Sign_Language_Translation_Perspectives.pdf">(paper)</a></li><br>
				
				<li> <i>Mixed Abilities and Varied Experiences: A Group Autoethnography of a Virtual Summer Internship</i> <br>
					K. Mack, M. Das, D. Jain, <b>D. Bragg</b>, J. Tang, A. Begel, E. Benetau, J. Davis, A. Glasser, J. Park, V. Potluri. 
					Communications of the ACM 2023. <a href="https://dl.acm.org/doi/pdf/10.1145/3604622">(article)</a></li><br>
				
				<li> <i>Tech Worker Perspectives on Considering the Interpersonal Impications of Communication Technologies</i> <br> E. Maris, K. Wagman, R. Bergmann, <b>D. Bragg</b>. GROUP 2023. <a href="papers/Tech Worker Perspectives.pdf">(paper)</a> </li><br>
				
				<li> <i>Exploring Team-Sourced Hyperlinks to Address Navigation Challenges for Low-Vision Readers of Scientific Papers</i> <br> S. Park, J. Bragg, M. Chang, K. Larson, <b>D. Bragg</b>. CSCW 2022. <a href="papers/Ocean.pdf">(paper)</a> </li><br>
				
				<li> <i>Exploring Collection of Sign Language Videos through Crowdsourcing</i> <br> <b>D. Bragg</b>, A. Glasser, F. Minakov, N. Caselli, W. Thies. CSCW 2022. <span class="badge badge-award">Impact Award</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/Exploring_Collection_of_Sign_Language_Videos_through_Crowdsourcing.pdf">(paper)</a> </li><br>
				
				
				<li> <i>ASL Wiki: An Exploratory Interface for Crowdsourcing ASL Translations</i> <br> A. Glasser, F. Minakov, <b>D. Bragg</b>. ASSETS 2022. <a href="papers/ASL_Wiki.pdf">(paper)</a> </li><br>
				
				<li> <i>Mixed Abilities and Varied Experiences: A Group Autoethnography of a Virtual Summer Internship</i> <br>
				K. Mack, M. Das, D. Jain, <b>D. Bragg</b>, J. Tang, A. Begel, E. Benetau, J. Davis, A. Glasser, J. Park, V. Potluri. 
				ASSETS 2021. <span class="badge badge-award">Best Paper Nominee</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/08/mack_etal_mixed_abilities_autoethno.pdf">(paper)</a> 
				
				
				<li> <i>Expanding a Large Inclusive Study of Human Listening Rates</i> <br>
				<b>D. Bragg</b>, K. Reinecke, R. Ladner. 
				TACCESS 2021. <a href="papers/Expanding_a_Large_Inclusive_Study.pdf">(paper)</a> </li><br>
				
				<li> <i>The FATE Landscape of Sign Language AI Datasets: An Interdisciplinary Perspective</i> <br>
				<b>D. Bragg</b>, N. Caselli, J. Hochgesang, M. Huenerfauth, L. Katz-Hernandez, O. Koller, R. Kushalnagar, C. Vogler, R. Ladner. 
				TACCESS 2021. <a href="papers/The_FATE_Landscape.pdf">(paper)</a> </li><br>
				
				
				<li> <i>ASL Sea Battle: Gamifying Sign Language Data Collection</i> <br>
				<b>D. Bragg</b>, N. Caselli, J. W. Gallagher, M. Goldberg, C. Oka, W. Thies. CHI 2021. <span class="badge badge-award">Honorable Mention</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/ASL_Sea_Battle.pdf">(paper)</a> </li><br>
				
				<li> <i>The Promise and Peril of Parallel Chat in Video Meetings for Work</i> <br>
				A. Sarkar, S. Rintel, D. Borowiec, R. Bergmann, S. Gillett, <b>D. Bragg</b>, N. Baym, A. Sellen. CHI LBW 2021. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/02/2021_CHI_LBW___parallel_chat___author_own_version_custom_footer.pdf">(paper)</a> </li><br>
				
				<li> <i>Designing an Online Infrastructure for Collecting AI Data From People With Disabilities</i> <br>
				J. Park, <b>D. Bragg</b>, E. Kamar., M.R. Morris. FAccT 2021. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/01/Inclusive_AI_Datasets_FINAL.pdf">(paper)</a></li><br>
				
				<li> <i>Exploring Collection of Sign Language Datasets: Privacy, Participation, and Model Performance</i> <br>
				<b>D. Bragg</b>, O. Koller, N. Caselli, W. Thies. 
				ASSETS 2020. <a href="papers/Exploring_Collection_of_Sign_Language_Datasets.pdf">(paper)</a> </li><br>
				
				<li> <i>Chat in the Hat: A Portable Interpreter for Sign Language Users</i> <br> 
				L. Berke, W. Thies, <b>D. Bragg</b>.
				ASSETS 2020.  <a href="papers/Chat_in_the_Hat.pdf">(paper)</a> </li><br>
				
				<li> <i>Social App Accessibility for Deaf Signers</i> <br>
				K. Mack, <b>D. Bragg</b>, M.R. Morris, M. Boss, I. Albi, A. Monroy-Hernandez.
				CSCW 2020. <a href="papers/Social_App_Accessibility_for_Deaf_Signers.pdf">(paper)</a> </li><br>
								
				<li> <i>Sign Language Interfaces: Discussing the Field's Biggest Challenges</i> <br>
				<b>D. Bragg</b>, M.R. Morris, C. Vogler, R. Kushalnagar, M. Huenerfauth, H. Kacorri.
				CHI 2020. <a href="papers/Sign_Language_Interfaces.pdf">(extended abstract)</a> </li><br>
				
				<li> <i>Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective</i> <br>
				<b>D. Bragg</b>, O. Koller, M. Bellard, L. Berke, P. Boudrealt, A. Braffort, N. Caselli, M. Huenerfauth, H. Kacorri, T. Verhoef, C. Vogler, M.R. Morris. 
				ASSETS 2019. <span class="badge badge-award">Best Paper</span> <img src="trophy.svg" alt="trophy icon" style="height: 2em;"> <a href="papers/Sign_Language_Workshop.pdf">(paper)</a> <a href="https://arxiv.org/pdf/1908.08597.pdf">(pre-print)</a> </li><br>
				
				<li> <i> Designing an Animated Character System for American Sign Language</i> <br>
				<b>D. Bragg</b>, R. Kushalnagar, R. Ladner. 
				ASSETS 2018. <a href="papers/Animated_Si5s.pdf">(paper)</a> <small> <b> <--- NOTE: Open with Adobe Reader to see animations <i>in the PDF itself!</i> </b> </small>  </li> </li><br>
									
				<li> <i> A Large Inclusive Study of Human Listening Rates</i> <br>
				<b>D. Bragg</b>, C. Bennett, K. Reinecke, R. Ladner.
				CHI 2018. <a href="papers/listening_rates.pdf">(paper)</a> </li><br>
				
				<li> <i>Designing and Evaluating Livefonts</i><br>
				 <b>D. Bragg</b>, S. Azenkot, K. Larson, A. Bessemans, A. Kalai.
				 UIST 2017. <a href="papers/livefonts.pdf">(paper)</a> <small> <b> <--- NOTE: Open with Adobe Reader to see animations <i>in the PDF itself!</i> </b> </small>  </li><br>
				
				<li> <i>Reading and Learning Smartfonts</i> <br>
				 <b>D. Bragg</b>, S. Azenkot, A. Kalai. 
				 UIST 2016. <a href="papers/Reading_and_Learning_Smartfonts.pdf">(paper)</a> </li><br>
				
				<li> <i> A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users</i><br>
				<b>D. Bragg</b>, N. Huynh, R. Ladner. 
				ASSETS 2016. <a href="papers/Personalizable_Mobile_Sound_Detector.pdf"> (paper) </a> </li><br>
				
				<li> <i> A User-Powered American Sign Language Dictionary</i><br>
				<b>D. Bragg</b>, K. Rector, R. Ladner. 
				CSCW 2015. <a href="papers/A_User-Powered_American_Sign_Language_Dictionary.pdf"> (paper) </a> </li><br>
			
				<li> <i> Synchronous Data Flow Modeling for DMIs</i><br>
				<b>D. Bragg</b>. 
				NIME 2013. <a href="papers/Synchronous_Data_Flow_Modeling.pdf" > (paper) </a> </li><br>

				<!--<li> Bragg, D., M. Yun, H. Bragg, H-A. Choi. “Intelligent Transmission of Patient Sensor Data in Wireless Hospital Networks.” Proc. AMIA (American Medical Informatics Association) Symposium. Chicago, Illinois. Nov 2012. <a href="papers/Intelligent_Transmission.pdf" > (paper) </a> </li>-->
				
				<!--<li> Bragg, D., M. Yun, H. Bragg, and H.A. Choi. “Game Theoretical Approach to Scheduling Transmission of Data for Hospital Patients.” Tel Aviv International Workshop on Game Theory. Tel Aviv, Israel. June 2011. (poster) </li>-->
				
				<li> <i> Battle Event Detection Using Sensor Networks and Distributed Query Processing</i><br>
				M. Yun, <b>D. Bragg</b>, A. Arora, and H.A. Choi.
				IEEE INFOCOM 2011. <a href="papers/Battle_Event_Detection.pdf" > (paper) </a> </li><br>
				
				<li> <i> On Data Transmission Scheduling considering Switching Penalty in Mobile Sensor Networks</i><br>
				Y. Zhou, <b>D. Bragg</b>, M. Yun, and H.A. Choi.
				IEEE INFOCOM 2011. <a href="papers/On_Data_Transmission_Scheduling.pdf" > (paper) </a> </li><br>

				<li> <i> Quantification and Display of Emotions in Music</i><br>
				<b>D. Bragg</b>. 
				Honors Senior Thesis, Harvard University Department of Applied Mathematics, 2010. <a href="papers/Bragg_Thesis.pdf" > (undergraduate thesis) </a> </li><br>

				<li> <i> Improving QoS in BitTorrent-like VoD Systems</i><br>
				Y. Yang, A. Chow, L. Golubchik, and <b>D. Bragg</b>. 
				IEEE INFOCOM 2010. <a href="papers/Imrpoving_QoS_in_BitTorrent-like_VoD_Systems.pdf" > (paper) </a> </li>

			</ul>
		</div>
		
		<!--
		<div class = "subtitle"> TEACHING </div>
		<br>
		
		<div class = "majorTextBox">
			 <a dataquery="#Link14iu" href="http://courses.cs.washington.edu/courses/cse446/13sp/">CSE 446: Machine Learning, 2013</a> - undergraduate Machine Learning course
		</div>
		-->


		<div class="centering">
		<h2 style="font-size: 16pt; font-weight: bold;"> CONTACT DANIELLE </h2>
		<a href="mailto:danielle.bragg@microsoft.com" title="Email me!"> Click to email me. </a>
		</div>
		<br>
	</div>

</body>
</html>
